{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fb94b1",
   "metadata": {},
   "source": [
    "<h1>Logistic Regression for prediction of leading role gender in movies</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feeb120",
   "metadata": {},
   "source": [
    "<h2>Import necessary libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.linear_model as slm\n",
    "import sklearn.model_selection as sms\n",
    "import sklearn.metrics as sm\n",
    "import sklearn.preprocessing as spp\n",
    "import sklearn.pipeline as spl\n",
    "import sklearn\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import sklearn.base as base\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc8654d",
   "metadata": {},
   "source": [
    "<h2>Some useful functions</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7548c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For removal of outliers, as a part of the sklearn pipeline process\n",
    "class WithoutOutliersClassifier(base.BaseEstimator, base.ClassifierMixin):\n",
    "    def __init__(self, outlier_detector, classifier, remove_outliers=True):\n",
    "        self.outlier_detector = outlier_detector\n",
    "        self.classifier = classifier\n",
    "        self.remove_outliers = remove_outliers   # Provides an easy way to run without outlier removal\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.outlier_detector_ = base.clone(self.outlier_detector)\n",
    "        if self.remove_outliers:\n",
    "            mask = self.outlier_detector_.fit_predict(X, y) == 1\n",
    "            self.classifier_ = base.clone(self.classifier).fit(X[mask], y[mask])\n",
    "        else:\n",
    "            self.classifier_ = base.clone(self.classifier).fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.classifier_.predict(X)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        return self.classifier_.decision_function(X)\n",
    "        #return self.predict_proba(X)[:,1]\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.classifier_.predict_proba(X)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ae20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def preprocess(dataset, add_features=None, keep_features=\"all\"):\n",
    "    \n",
    "    train_data = dataset.copy()\n",
    "\n",
    "    if keep_features != \"all\":\n",
    "        features = keep_features\n",
    "        features.append(LABEL_COLUMN)\n",
    "        train_data = train_data[features]\n",
    "\n",
    "    if add_features == \"ratio_female\":\n",
    "        train_data.insert(len(train_data.columns)-1, column=\"Ratio female actors\", value=dataset[\"Number of female actors\"] / (dataset[\"Number of male actors\"] + dataset[\"Number of female actors\"]))\n",
    "    \n",
    "    if add_features == \"add_ratios\": \n",
    "        train_data.insert(len(train_data.columns)-1, column=\"Ratio female actors\", value=dataset[\"Number of female actors\"] / (dataset[\"Number of male actors\"] + dataset[\"Number of female actors\"]))\n",
    "        train_data.insert(len(train_data.columns)-1, column=\"Ratio female words\", value=dataset[\"Number words female\"] / dataset[\"Total words\"])\n",
    "        train_data.insert(len(train_data.columns)-1, column=\"Ratio lead words\", value=dataset[\"Difference in words lead and co-lead\"] / dataset[\"Total words\"])\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a1cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn pipeline with outlier removal and scaling\n",
    "def create_pipeline(lr_model, remove_outliers=True, use_scaling=True):\n",
    "    scaler = spp.StandardScaler()\n",
    "    ol = LocalOutlierFactor(n_neighbors=2)\n",
    "    #ol = IsolationForest(n_estimators=10, warm_start=True)\n",
    "    #ol = OneClassSVM()\n",
    "    woc = WithoutOutliersClassifier(ol, lr_model, remove_outliers)\n",
    "    \n",
    "    if use_scaling:\n",
    "        p = [(\"preprocess\", scaler)]\n",
    "    else:\n",
    "        p = []\n",
    "    \n",
    "    p.append((\"lr\", woc))\n",
    "    \n",
    "    return spl.Pipeline(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune parameters for the model, with outlier removal as part of the pipeline\n",
    "def tune(lr_model, X_train, y_train, R=5, K=10):\n",
    "\n",
    "    # Set up the CV grid search for tuning\n",
    "    lambda_exp_min = -1\n",
    "    lambda_exp_max = 2\n",
    "    search_range = {\n",
    "                     \"lr__classifier__C\":[1/(10**x) for x in np.linspace(lambda_exp_min, lambda_exp_max, 40)],\n",
    "                     \"lr__classifier__penalty\":[\"l1\", \"l2\"]\n",
    "                   }\n",
    "    \n",
    "    rkf = sms.RepeatedStratifiedKFold(n_repeats=R, n_splits=K, random_state=17)\n",
    "\n",
    "    pipe = create_pipeline(lr_model, remove_outliers=True, use_scaling=True)\n",
    "    \n",
    "    grid_search = sms.GridSearchCV(estimator=pipe,\n",
    "                                   param_grid=search_range,\n",
    "                                   cv=rkf,\n",
    "                                   scoring=\"accuracy\",\n",
    "                                   return_train_score=True,\n",
    "                                   verbose=1)\n",
    "\n",
    "    # Search for the best lambda, using repeated 10-fold cross-valiation to evaluate each lambda value\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Return the result:\n",
    "    \n",
    "    # 1. The best model found during the tuning and\n",
    "    best_lr_model = grid_search.best_estimator_.named_steps[\"lr\"].classifier_\n",
    "\n",
    "    # 2. The trainset and testset scores, for evaluating the suitable hyperparameter ranges. We do this for both L1 and L2 regularisation.\n",
    "    scores = grid_search.cv_results_\n",
    "    \n",
    "    return (best_lr_model, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693103b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract scores from the complicated grid search cv result structure\n",
    "def get_tuning_scores(res, penalty, lambdas_key, scores_key):\n",
    "    return (res[lambdas_key][res[\"param_lr__classifier__penalty\"]==penalty].astype(\"float\"),\n",
    "            res[scores_key] [res[\"param_lr__classifier__penalty\"]==penalty])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6632cdc4",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44873277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data\n",
    "LABEL_COLUMN = \"Lead\"\n",
    "raw_data = pd.read_csv(\"train.csv\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ratio of movies with female lead:\", len(raw_data[raw_data[\"Lead\"]==\"Female\"])/len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d98609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(raw_data, add_features=\"add_ratios\")\n",
    "\n",
    "#train_data = preprocess(raw_data, keep_features=[], add_features=\"add_ratios\")\n",
    "\n",
    "X_train = train_data.loc[:, train_data.columns != LABEL_COLUMN].values\n",
    "y_train = train_data.loc[:, train_data.columns == LABEL_COLUMN].values.ravel()\n",
    "X = X_train\n",
    "y = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6d4002d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number words female</th>\n",
       "      <th>Total words</th>\n",
       "      <th>Number of words lead</th>\n",
       "      <th>Difference in words lead and co-lead</th>\n",
       "      <th>Number of male actors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Number of female actors</th>\n",
       "      <th>Number words male</th>\n",
       "      <th>Gross</th>\n",
       "      <th>Mean Age Male</th>\n",
       "      <th>Mean Age Female</th>\n",
       "      <th>Age Lead</th>\n",
       "      <th>Age Co-Lead</th>\n",
       "      <th>Ratio female actors</th>\n",
       "      <th>Ratio female words</th>\n",
       "      <th>Ratio lead words</th>\n",
       "      <th>Lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512</td>\n",
       "      <td>6394</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>343</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>5</td>\n",
       "      <td>2631</td>\n",
       "      <td>142.0</td>\n",
       "      <td>51.500000</td>\n",
       "      <td>42.333333</td>\n",
       "      <td>46.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.236472</td>\n",
       "      <td>0.053644</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524</td>\n",
       "      <td>8780</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1219</td>\n",
       "      <td>9</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>5236</td>\n",
       "      <td>37.0</td>\n",
       "      <td>39.125000</td>\n",
       "      <td>29.333333</td>\n",
       "      <td>58.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.173576</td>\n",
       "      <td>0.138838</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>155</td>\n",
       "      <td>4176</td>\n",
       "      <td>942.0</td>\n",
       "      <td>787</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>3079</td>\n",
       "      <td>376.0</td>\n",
       "      <td>42.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.037117</td>\n",
       "      <td>0.188458</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1073</td>\n",
       "      <td>9855</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>2623</td>\n",
       "      <td>12</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>5342</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.222222</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.108879</td>\n",
       "      <td>0.266159</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1317</td>\n",
       "      <td>7688</td>\n",
       "      <td>3835.0</td>\n",
       "      <td>3149</td>\n",
       "      <td>8</td>\n",
       "      <td>1988</td>\n",
       "      <td>4</td>\n",
       "      <td>2536</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.250000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.171306</td>\n",
       "      <td>0.409599</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>303</td>\n",
       "      <td>2398</td>\n",
       "      <td>1334.0</td>\n",
       "      <td>1166</td>\n",
       "      <td>5</td>\n",
       "      <td>1973</td>\n",
       "      <td>2</td>\n",
       "      <td>761</td>\n",
       "      <td>174.0</td>\n",
       "      <td>43.200000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.126355</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>632</td>\n",
       "      <td>8404</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>187</td>\n",
       "      <td>6</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "      <td>5820</td>\n",
       "      <td>172.0</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>21.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>1326</td>\n",
       "      <td>2750</td>\n",
       "      <td>877.0</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>547</td>\n",
       "      <td>53.0</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>27.666667</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.482182</td>\n",
       "      <td>0.129455</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>462</td>\n",
       "      <td>3994</td>\n",
       "      <td>775.0</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>1996</td>\n",
       "      <td>3</td>\n",
       "      <td>2757</td>\n",
       "      <td>32.0</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>38.500000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.115674</td>\n",
       "      <td>0.013020</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>2735</td>\n",
       "      <td>11946</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>13</td>\n",
       "      <td>2007</td>\n",
       "      <td>4</td>\n",
       "      <td>5801</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.090909</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.228947</td>\n",
       "      <td>0.128579</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1039 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Number words female  Total words  Number of words lead  \\\n",
       "0                    1512         6394                2251.0   \n",
       "1                    1524         8780                2020.0   \n",
       "2                     155         4176                 942.0   \n",
       "3                    1073         9855                3440.0   \n",
       "4                    1317         7688                3835.0   \n",
       "...                   ...          ...                   ...   \n",
       "1034                  303         2398                1334.0   \n",
       "1035                  632         8404                1952.0   \n",
       "1036                 1326         2750                 877.0   \n",
       "1037                  462         3994                 775.0   \n",
       "1038                 2735        11946                3410.0   \n",
       "\n",
       "      Difference in words lead and co-lead  Number of male actors  Year  \\\n",
       "0                                      343                      2  1995   \n",
       "1                                     1219                      9  2001   \n",
       "2                                      787                      7  1968   \n",
       "3                                     2623                     12  2002   \n",
       "4                                     3149                      8  1988   \n",
       "...                                    ...                    ...   ...   \n",
       "1034                                  1166                      5  1973   \n",
       "1035                                   187                      6  1992   \n",
       "1036                                   356                      2  2000   \n",
       "1037                                    52                      8  1996   \n",
       "1038                                  1536                     13  2007   \n",
       "\n",
       "      Number of female actors  Number words male  Gross  Mean Age Male  \\\n",
       "0                           5               2631  142.0      51.500000   \n",
       "1                           4               5236   37.0      39.125000   \n",
       "2                           1               3079  376.0      42.500000   \n",
       "3                           2               5342   19.0      35.222222   \n",
       "4                           4               2536   40.0      45.250000   \n",
       "...                       ...                ...    ...            ...   \n",
       "1034                        2                761  174.0      43.200000   \n",
       "1035                        2               5820  172.0      37.166667   \n",
       "1036                        3                547   53.0      27.500000   \n",
       "1037                        3               2757   32.0      42.857143   \n",
       "1038                        4               5801   32.0      44.090909   \n",
       "\n",
       "      Mean Age Female  Age Lead  Age Co-Lead  Ratio female actors  \\\n",
       "0           42.333333      46.0         65.0             0.714286   \n",
       "1           29.333333      58.0         34.0             0.307692   \n",
       "2           37.000000      46.0         37.0             0.125000   \n",
       "3           21.500000      33.0         23.0             0.142857   \n",
       "4           45.000000      36.0         39.0             0.333333   \n",
       "...               ...       ...          ...                  ...   \n",
       "1034        31.000000      46.0         24.0             0.285714   \n",
       "1035        24.000000      21.0         34.0             0.250000   \n",
       "1036        27.666667      28.0         25.0             0.600000   \n",
       "1037        38.500000      29.0         32.0             0.272727   \n",
       "1038        50.000000      38.0         48.0             0.235294   \n",
       "\n",
       "      Ratio female words  Ratio lead words    Lead  \n",
       "0               0.236472          0.053644  Female  \n",
       "1               0.173576          0.138838    Male  \n",
       "2               0.037117          0.188458    Male  \n",
       "3               0.108879          0.266159    Male  \n",
       "4               0.171306          0.409599    Male  \n",
       "...                  ...               ...     ...  \n",
       "1034            0.126355          0.486239    Male  \n",
       "1035            0.075202          0.022251  Female  \n",
       "1036            0.482182          0.129455    Male  \n",
       "1037            0.115674          0.013020  Female  \n",
       "1038            0.228947          0.128579    Male  \n",
       "\n",
       "[1039 rows x 17 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c8037",
   "metadata": {},
   "source": [
    "<h2>Tuning and training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for suitable hyper parameters, compare L1 and L2, and check whether overfit/underfit may occur\n",
    "lr = slm.LogisticRegression(solver='liblinear')\n",
    "\n",
    "result = tune(lr, X_train, y_train, R=5, K=10)\n",
    "\n",
    "best_model = result[0]\n",
    "scores = result[1]\n",
    "\n",
    "res_train_L1 = get_tuning_scores(scores, \"l1\", \"param_lr__classifier__C\", \"mean_train_score\")\n",
    "res_test_L1  = get_tuning_scores(scores, \"l1\", \"param_lr__classifier__C\", \"mean_test_score\")\n",
    "res_train_L2 = get_tuning_scores(scores, \"l2\", \"param_lr__classifier__C\", \"mean_train_score\")\n",
    "res_test_L2  = get_tuning_scores(scores, \"l2\", \"param_lr__classifier__C\", \"mean_test_score\")\n",
    "\n",
    "plt.plot(np.log10(1/res_train_L1[0]), 1-res_train_L1[1], label=\"Train set error, L1\")\n",
    "plt.plot(np.log10(1/res_test_L1[0]), 1-res_test_L1[1], label=\"Test set error, L1\")\n",
    "plt.plot(np.log10(1/res_train_L2[0]), 1-res_train_L2[1], label=\"Train set error, L2\")\n",
    "plt.plot(np.log10(1/res_test_L2[0]), 1-res_test_L2[1], label=\"Test set error, L2\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"log10(lambda)\")\n",
    "plt.ylabel(\"Misclassification\")\n",
    "plt.title(\"Misclassification ratio for different values of lambda\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d748b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of individual feature importance\n",
    "weights = pd.DataFrame({\"Feature\":train_data.columns[:-1].values, \"Weight\":best_model.coef_[0]})\n",
    "weights = weights.sort_values(by=\"Weight\", key=lambda w:-abs(w), ignore_index=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f98c9",
   "metadata": {},
   "source": [
    "<h2>Model evaluation for comparison</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d9617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model that was found, using a variety of metrics\n",
    "rkf = sms.RepeatedStratifiedKFold(n_repeats=5, n_splits=10, random_state=17)\n",
    "pipe = create_pipeline(best_model)\n",
    "metrics = {\n",
    "           \"accuracy\":\"accuracy\",\n",
    "           \"f1\": sm.make_scorer(sm.f1_score, pos_label=\"Male\"),\n",
    "           \"roc_auc\":\"roc_auc\",\n",
    "           \"precision\":sm.make_scorer(sm.precision_score, pos_label=\"Male\"),\n",
    "           \"recall\":sm.make_scorer(sm.recall_score, pos_label=\"Male\")\n",
    "          }\n",
    "result = sms.cross_validate(pipe, X_train, y_train, scoring=metrics, cv=rkf)\n",
    "\n",
    "labels = []\n",
    "avg = []\n",
    "std = []\n",
    "\n",
    "for m in metrics:\n",
    "    values = result[\"test_\" + m]\n",
    "    labels.append(m)\n",
    "    avg.append(np.mean(values))\n",
    "    std.append(np.std(values))\n",
    "    \n",
    "pd.DataFrame({\"Metric\":labels, \"Average\":avg, \"Std dev\":std})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cdf106",
   "metadata": {},
   "source": [
    "<h2>Intermediate results</h2>\n",
    "<p>\n",
    "Different combinations of preprocessing steps and feature engineering options were tried. In each case, the workflow above was used to find the optimal hyper parameters and then evaluating the resulting model on the entire dataset. Since the results looked most promising when including the feature interactions with ratios of female actors, female words and lead words, this configuration was selected. The removal of outliers had very little effect but was also included in the final configuration. Some of the results can be found in the table below (which shows the resulting accuracy with standard deviation).\n",
    "</p>\n",
    "<table align=\"left\">\n",
    "    <tr><td>Using all features (with standardization) plus \"three ratios\" plus remove outliers (LOF)</td><td>0.893 +/- 0.026</td></tr>\n",
    "    <tr><td>Using all features (with standardization) plus \"three ratios\"</td><td>0.890 +/- 0.026</td></tr>\n",
    "    <tr><td>Using only the \"three ratios\"</td><td>0.875 +/- 0.029</td></tr>\n",
    "    <tr><td>Using all features (with standardization) plus ratio female</td><td>0.873 +/- 0.031</td></tr>\n",
    "    <tr><td>Using all features with standardization</td><td>0.873 +/- 0.031</td></tr>\n",
    "    <tr><td>Using all features without preprocessing</td><td>0.853 +/- 0.028</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5bb75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
